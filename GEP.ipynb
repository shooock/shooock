{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Productivity Prediction of Garment Employees\n",
    "\n",
    "\n",
    "\n",
    "- Family Name:Xuan\n",
    "- Given Name:Tianyuan\n",
    "- email:irvingxuan@gmail.com\n",
    "\n",
    "- Last edited date: 3/9/2021\n",
    "\n",
    "Programming Language: R 3.7 in Jupyter Notebook\n",
    "\n",
    "R Libraries used:\n",
    "\n",
    "- ggplot2\n",
    "\n",
    "- reshape2\n",
    "\n",
    "- car\n",
    "\n",
    "- stats\n",
    "\n",
    "- scales\n",
    "\n",
    "- grid\n",
    "\n",
    "- gridExtra\n",
    "\n",
    "- RColorBrewer\n",
    "\n",
    "- lattice\n",
    "\n",
    "- caret\n",
    "\n",
    "- xgboost\n",
    "\n",
    "- plyr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#sec_1)\n",
    "3. [Exploratory Data Analysis](#sec_3)\n",
    "3. [Methodology](#sec_4)\n",
    "3. [Model Development](#sec_5)\n",
    "3. [Results and discussion](#sec_6)\n",
    "3. [Conclusion](#sec_7)\n",
    "3. [References](#sec_8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a class=\"anchor\" id=\"sec_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file \"garments_empolyee_productivity.csv\"consist of data relating to modern garments industry productivity.This file contains 14 attributes, one target value, and more than a thousand pieces of data. The data is first analyzed in order to decide what method of machine learning will be used. After the analysis, it was found that the training features were first transformed, and the different models were optimized by adding or filtering the features to get a better model. In addition to the common linear regression, logistic, KNN, and Decision Tree are also used. By comparing different models, a better model for predicting actual_productivity is finally obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis<a class=\"anchor\" id=\"sec_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(reshape2)\n",
    "library(car)\n",
    "library(stats)\n",
    "library(scales)\n",
    "library(grid)\n",
    "library(gridExtra)\n",
    "library(RColorBrewer)\n",
    "library(lattice)\n",
    "library(caret)\n",
    "library(xgboost)\n",
    "library(plyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview of the whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "GEP <- read.csv(\"garments_empolyee_productivity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the dimensions\n",
    "cat(\"The garments_empolyee_productivity has\", dim(GEP)[1], \"records, each with\", dim(GEP)[2],\n",
    "    \"attributes. The structure is:\\n\\n\")\n",
    "\n",
    "# Display the structure\n",
    "str(GEP)\n",
    "\n",
    "cat(\"\\nThe first few and last few records in the dataset are:\")\n",
    "# Inspect the first few records\n",
    "head(GEP)\n",
    "# And the last few\n",
    "tail(GEP)\n",
    "\n",
    "cat(\"\\nBasic statistics for each attribute are:\")\n",
    "# Statistical summary \n",
    "summary(GEP)\n",
    "\n",
    "cat(\"The numbers of unique values for each attribute are:\")\n",
    "apply(GEP, 2, function(x) length(unique(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly, 70% of the data are used as trian dataset and 30% of the data are used as test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(101) # Set Seed so that same sample can be reproduced in future also\n",
    "# Now Selecting 70% of data as sample from total 'n' rows of the data  \n",
    "sample <- sample.int(n = nrow(GEP), size = floor(.7*nrow(GEP)), replace = F)\n",
    "train <- GEP[sample, ]\n",
    "test  <- GEP[-sample,]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(train)\n",
    "head(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Attributes\n",
    "\n",
    "The following table identifies which attributes are numerical and whether they are continuous or discrete, and which are categorical and whether they are nominal or ordinal. It includes some initial observations about the ranges and common values of the attributes.\n",
    "\n",
    "|Attribute  |Type       |Sub-type  |Comments                                                                              |\n",
    "|-----------|-----------|----------|--------------------------------------------------------------------------------------|\n",
    "|date        |Categorical|Ordinal    |Date in MM-DD-YYYY, probably has no corelation with target value.|\n",
    "|day      |Categorical  |Ordinal|Day of the Week, probably has no corelation with target value.| \n",
    "|quarter   |Categorical  |Ordinal |A portion of the month. A month was divided into four quarters. There are also a few quater 5              |\n",
    "|department  |Categorical  |Nominal  |Associated department with the instance and has 2 classed finishing, sweing|\n",
    "|no_of_workers| Numerical   |Discrete| Associated team number with the instance.  Ranges from 2 to 89 - could have  outliers                                     |\n",
    "|no_of_style_change   |Numerical  |Continuous| Number of changes in the style of a particular product. Majority of data are 0.       |\n",
    "| targeted_productivity |Numerical|Continuous   |Targeted productivity set by the Authority for each team for each day.                   |\n",
    "|smv  |Numerical |Continuous   |Standard Minute Value, it is the allocated time for a task with min 2.9 and max 54.56. It could have outliers    |\n",
    "|wip       |Numerical |Continuous   |Work in progress. Includes the number of unfinished items for products. It could have outliers |\n",
    "|over_time  |Numerical |Continuous   |Represents the amount of overtime by each team in minutes. Range from 0 to 25920      |\n",
    "|incentive   |Numerical |Continuous    |Has 48 values between 0 and 3600, whose mean is 38.21. It could have extreme outliers. |\n",
    "|idle_time   |Numerical |Continuous    |The amount of time when the production was interrupted due to several reasons. Range is from 0 to 300, majority of it is 0.\n",
    "|idle_men   |Numerical |Continuous    |The number of workers who were idle due to production interruption. Range is from 0 to 45 majority of it is 0.|\n",
    "|actual_productivity   |Numerical |Continuous    |The actual % of productivity that was delivered by the workers. It ranges from 0-1. Our target attribute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Distribution of Each Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "attach(GEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the variable distributions using boxplots(numberical attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that here we are observing the distribution of numerical type data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate box plots of all variables except the date and department\n",
    "boxplot <- melt(as.data.frame(GEP[,c(-0,-3)]))\n",
    "ggplot(boxplot,aes(x = variable,y = value)) +\n",
    "facet_wrap(~variable, scales=\"free\") +\n",
    "geom_boxplot() +\n",
    "scale_y_continuous(labels=function (n) {format(n, scientific=FALSE)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the variable distributions using histograms and bar charts\n",
    "### num and str type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram or bar chart of each variable\n",
    "par(mfrow = c(4,3))\n",
    "hist(no_of_workers)\n",
    "hist(no_of_style_change)\n",
    "hist(targeted_productivity)\n",
    "hist(smv)\n",
    "hist(wip)\n",
    "hist(incentive)\n",
    "hist(idle_time)\n",
    "hist(idle_men)\n",
    "hist(actual_productivity)\n",
    "hist(team)\n",
    "hist(over_time)\n",
    "\n",
    "par(fig=c(0,1,0,0.30),ps=10,new=TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barplot(sort(table(actual_productivity)),las=2,main=\"Bar Chart of actual_productivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow = c(2,2))\n",
    "plot(as.factor(date),main=\"Bar Chart of date\")\n",
    "plot(as.factor(day), main=\"Bar Chart of day\")\n",
    "plot(as.factor(quarter), main=\"Bar Chart of quarter\")\n",
    "plot(as.factor(department), main=\"Bar Chart of department\")\n",
    "par(fig=c(0,1,0,0.30),ps=10,new=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs show:\n",
    "- wip, incentive, idle_time, idle_men all have large positive skews.\n",
    "- wip, incentive, idle_time, idle_men,wip,no_of_style_changes have a lot of zero value.\n",
    "- Most incentive are with value 50,although it has value more than one thousand.\n",
    "- Team one has more work tasks than other teams while other teams are probably equal.\n",
    "- very few data with quarter 5\n",
    "- mojority of department are sweing\n",
    "- If we look at boxplot alone, many data have exaggerated outliers, so these outliers are also important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a closer look at some interesting features\n",
    "\n",
    "Replot wip,incentive,idle_time using a log scale to see if these variables have a log-normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some colours using Colorbrewer\n",
    "gg.colour <- brewer.pal(12,\"Paired\")[12]\n",
    "gg.fill <- brewer.pal(12,\"Paired\")[11]\n",
    "\n",
    "# Re-plot some of the charts using log scales to counteract the skew\n",
    "p1 <- ggplot(aes(x=wip), data=GEP) +\n",
    "      geom_histogram(bins=20, colour=gg.colour, fill=gg.fill) +\n",
    "      scale_x_log10(labels=comma) \n",
    "p2 <- ggplot(aes(x=incentive), data=GEP) +\n",
    "      geom_histogram(bins=20, colour=gg.colour, fill=gg.fill) +\n",
    "      scale_x_log10(labels=comma)\n",
    "p3 <- ggplot(aes(x=smv), data=GEP) +\n",
    "      geom_histogram(bins=20, colour=gg.colour, fill=gg.fill) +\n",
    "      scale_x_log10(labels=comma)\n",
    "\n",
    "grid.arrange(p1, p2,p3, ncol=1, nrow=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These graphs show:\n",
    "- The log of incentive has a little bit normally distributed,left part.\n",
    "- The log of the wip is not quite normal. The majority of lot sizes are between 100 and 10,000  wip, with a few outliers > 100,00 or <100 wip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Investigate Pairs of Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation Plot Function\n",
    "\n",
    "This is the DIY correlation plot provided in the tutorial.\n",
    "\n",
    "First we look at the overall correlation between variables and the distribution of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIY correlation plot\n",
    "# http://stackoverflow.com/questions/31709982/how-to-plot-in-r-a-correlogram-on-top-of-a-correlation-matrix\n",
    "# there's some truth to the quote that modern programming is often stitching together pieces from SO \n",
    "\n",
    "colorRange <- c('#69091e', '#e37f65', 'white', '#aed2e6', '#042f60')\n",
    "## colorRamp() returns a function which takes as an argument a number\n",
    "## on [0,1] and returns a color in the gradient in colorRange\n",
    "myColorRampFunc <- colorRamp(colorRange)\n",
    "\n",
    "panel.cor <- function(w, z, ...) {\n",
    "    correlation <- cor(w, z)\n",
    "\n",
    "    ## because the func needs [0,1] and cor gives [-1,1], we need to shift and scale it\n",
    "    col <- rgb(myColorRampFunc((1 + correlation) / 2 ) / 255 )\n",
    "\n",
    "    ## square it to avoid visual bias due to \"area vs diameter\"\n",
    "    radius <- sqrt(abs(correlation))\n",
    "    radians <- seq(0, 2*pi, len = 50) # 50 is arbitrary\n",
    "    x <- radius * cos(radians)\n",
    "    y <- radius * sin(radians)\n",
    "    ## make them full loops\n",
    "    x <- c(x, tail(x,n=1))\n",
    "    y <- c(y, tail(y,n=1))\n",
    "\n",
    "    ## trick: \"don't create a new plot\" thing by following the\n",
    "    ## advice here: http://www.r-bloggers.com/multiple-y-axis-in-a-r-plot/\n",
    "    ## This allows\n",
    "    par(new=TRUE)\n",
    "    plot(0, type='n', xlim=c(-1,1), ylim=c(-1,1), axes=FALSE, asp=1)\n",
    "    polygon(x, y, border=col, col=col)\n",
    "}\n",
    "\n",
    "# usage e.g.:\n",
    "# pairs(mtcars, upper.panel = panel.cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatterplot Matrix\n",
    "\n",
    "**1)** Plot the variables using a scatterplot matrix to visualise the correlations between variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs(GEP[sample.int(nrow(GEP),1000),], lower.panel=panel.cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the above table we can clearly observe that there are strong correlations between some variables.\n",
    "For example\n",
    "\n",
    "- smv， no_of_workers， over_time,department，there is a correlation between two of them.\n",
    "- Looking at the distribution graphs, it does appear that there is a linear distribution relationship between some of the data. But the picture is too small we need to investigate further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the distribution of some of the above variables through a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatterplotMatrix(~smv+no_of_workers+over_time+department+actual_productivity,data=GEP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the above image, the correlation between the data distribution between smv and no_of_workers, for example, is still very obvious. The correlation between targer_productivity and other variables is hard to identify. This is also consistent with the information obtained from the above table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Num values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis of correlation above, and the distribution relationship between variables, it is relatively easy to observe the relationship between numberic data. But for example, some dates, departments, and how these data are related to other data, we need to explore further.\n",
    "\n",
    "First let's look at numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(GEP[c(5,6,7,8,9,10,11,12,13,14,15)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate all Pearson's correlation for num cols\n",
    "Pearson_cor<- as.data.frame(cor(GEP[,c(5,6,7,8,9,10,11,12,13,14,15)]),method=\"pearson\")\n",
    "#Select 28th col which is every variable vurse actual_productivity\n",
    "Pearson_cor[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see just from the table above, there are no variables that have a particularly strong correlation with actual_productivity.So let's take a closer look at the exact value of the correlation between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Define you own panel\n",
    "myPanel <- function(x, y, z, ...) {\n",
    "    panel.levelplot(x,y,z,...)\n",
    "    panel.text(x, y, round(z, 2))\n",
    "}\n",
    "#Define the color scheme\n",
    "cols = colorRampPalette(c(\"red\",\"blue\"))\n",
    "#Plot the correlation matrix.\n",
    "levelplot(cor(GEP[c(5,6,7,8,9,10,11,12,13,14,15)]), col.regions = cols(100), main = \"correlation\", xlab = NULL, ylab = NULL, \n",
    "          scales = list(x = list(rot = 90)), panel = myPanel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing the above correlations it is obtained that:\n",
    "\n",
    "- As mentioned above smv， no_of_workers， over_time, no_of_style_change，there is a correlation between two of them.Having correlation means the absolute value of correlation is greater than 0.3(pupple and blue in figure) here.\n",
    "- targeted_productivity has a correlation with actual_productivity with 0.42.\n",
    "- There are no variables that have a particularly strong correlation with actual_productivity.\n",
    "- actual_productivity itself is a little bit near a Guassan distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### String values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then let's look at string data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=8,repr.plot.height=4)\n",
    "\n",
    "ggplot(aes(x=no_of_workers),data =GEP) + \n",
    "    geom_density(aes(fill = quarter)) +\n",
    "    facet_wrap(~as.factor(quarter)) +\n",
    "    ggtitle('number of workers and quarter Relationship')\n",
    "\n",
    "options(repr.plot.width=8,repr.plot.height=3)\n",
    "\n",
    "ggplot(aes(x=actual_productivity),data =GEP) + \n",
    "    geom_density(aes(fill = quarter)) +\n",
    "    facet_wrap(~as.factor(quarter)) +\n",
    "    ggtitle('actual_productivity and quarter Relationship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=8,repr.plot.height=3)\n",
    "\n",
    "ggplot(aes(x=no_of_workers),data =GEP) + \n",
    "    geom_density(aes(fill = department)) +\n",
    "    facet_wrap(~as.factor(department)) +\n",
    "    ggtitle('number of workers and department Relationship')\n",
    "\n",
    "options(repr.plot.width=8,repr.plot.height=3)\n",
    "\n",
    "ggplot(aes(x=log(incentive+1)),data =GEP) + \n",
    "    geom_density(aes(fill = department)) +\n",
    "    facet_wrap(~as.factor(department)) +\n",
    "    ggtitle('number of workers and department Relationship')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(repr.plot.width=8,repr.plot.height=3)\n",
    "\n",
    "ggplot(aes(x=actual_productivity),data =GEP) + \n",
    "    geom_density(aes(fill = department)) +\n",
    "    facet_wrap(~as.factor(department)) +\n",
    "    ggtitle('actual_productivity and department Relationship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing the above figures it is obtained that:\n",
    "\n",
    "- The number of workers varies slightly from quarter to quarter, and the distribution of workers from quarters 1 to 4 is roughly the same. However, we can clearly see the difference between quarter5 and the other four. quarter5 has more of a two Gaussian mixture distribution, while the others have a three Gaussian mixture distribution.\n",
    "- When the actual productivity rate is below 0.8, it can be seen that quarters 1 through 4 account for more of the data. And it is clear that most of the data belonging to quarter 5 have a productivity rate of more than 0.8.\n",
    "- Different departments obviously have different number of workers, and the number of workers in the sweing department is obviously more than that in the finishing state.\n",
    "- It is obvious that the product rate of data in the finishing state is higher than that of data in the sweing state.\n",
    "- Since the incentive values are difficult to observe, and as analyzed earlier, we take log for incentive+1 to get the distribution about the distribution department, it is obvious to find that the distribution of incentive values is obviously very different in different department states. In finishing it is mostly a value close to 0, while in sweing state it is a level with a mean of about 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology<a class=\"anchor\" id=\"sec_4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, because the actual productivity we need to predict is a number. In addition, this value is not classified data but a set of values. Obviously, regression will be used here. In this part we will use linear regression, logistc regression, KNN, and xgboost to perform machine learning on the data.\n",
    "\n",
    "They all have their own characteristics without considering changing the feature.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "- Methodology is simple, easy to implement\n",
    "- Can solve regression problems\n",
    "- It is very convenient for us to do feature engineering under this model.\n",
    "\n",
    "### Logistic Regression\n",
    "- Methodology is simple, easy to implement\n",
    "- Can solve regression problems\n",
    "- It is very convenient for us to do feature engineering under this model.\n",
    "\n",
    "### KNN\n",
    "- Methodology is simple, easy to implement\n",
    "- Can solve regression problems\n",
    "- High accuracy, no assumptions about data, insensitive to outlier\n",
    "\n",
    "### Extreme Gradient Boosting\n",
    "\n",
    "- Can solve regression problems\n",
    "- This is a mature function edited by machine learning scientists.\n",
    "- XGBoost adds a regular term to the cost function.\n",
    "- In short, all aspects of this method have been considered and may be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model perfomance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. MSE\n",
    "We use Mean Square Error(MSE) as error function here.\n",
    "It is simple and can express the pros and cons of the model to a certain extent.\n",
    "\\begin{align}\n",
    "M S E=\\frac{\\sum_{i=1}^{n}\\left(y_{i}-y_{i}^{p}\\right)^{2}}{n}\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "### 2.Residual standard error\n",
    "\n",
    "Since we are doing a regression, a good measure for judging how well the model is performing is RSE.\n",
    "\n",
    "It represents the sum of the squares of the difference between the true value and the predicted value divided by the degrees of freedom, and finally takes the root. Let's see its fomular.\n",
    "\n",
    "\\begin{equation}\n",
    "R S E=\\sqrt{\\frac{\\sum_{i=1}^{n}\\left(predicted-a c t u al\\right)^{2}}{df}}\n",
    "\\end{equation}\n",
    "\n",
    "In addition, there are MAE and RSME, which are similar to RSE but also have subtle differences. For example, MAE is not sensitive to outliers.\n",
    "\n",
    "### 3. R squared\n",
    "\n",
    "R squared represents the explanatory power of the model. For example, R squared of 0.5 means that 50% of the variance can be interpreted by the input. Of course, R squared is not as high as possible. Obviously, too high R squared may be caused by overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Development <a class=\"anchor\" id=\"sec_5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at the result of using lm directly on the original data.\n",
    "\n",
    "Pay special attention to that we use train dataset to train and test to check. In the first part, we have divided train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_fit1<- lm(actual_productivity~.,data= train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lm_fit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw.fit =step(lm_fit1,trace=0,k=log(nrow(train)), direction=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(sw.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow=c(4,4))\n",
    "plot(lm_fit1)\n",
    "plot(sw.fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After linear regression on the entire train dataset, we use step on it. We found that before and after pruning, the effect is worse after removing relatively unimportant variables.We can see the figures above. The distribution of residual, that is, the gap between the real value and the predicted value, does not seem to change. There seems to be no observable change in the shape of Q-Q norm,too. In observing the data, the Residual standard error has increased by 0.02 instead. Multiple R-squared has dropped instead. So we learned that we trimmed too many variables at this time, and we can't explain the predicted variance well, or it's related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By directly performing linear regression on the entire train dataset, we know that there are too many irrelevant variables. When we keep only the variables with strong correlation, we will find that the model's ability to explain the predicted value is weakening. So we have to add some variables. From the EDA part we have done above, we know that many variables are directly and indirectly related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_fit2<- update(sw.fit,.~ .+log(wip)+log(incentive+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lm_fit2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compared with sw_fit, R squared is improved, indicating that we can better explain the variance of the target.\n",
    "- Compared with lm_fit1, F-statistic has been greatly improved. This shows that in the new model, the overall correlation between the selected variables and the target variables is stronger.\n",
    "- Compared with lm_fit1, Adjusted R-squared has increased,we can say that our newly added variables improve the model.\n",
    "- When we observe log(incentive+1) carefully, we find that its p-value is quite small. This shows that we have no reason to say that it has nothing to do with actual_productivity in hypothesis testing.\n",
    "- we need consider that if we keep log(wip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_fit3<- update(lm_fit2,.~ .+quarter:no_of_workers+department:no_of_workers+smv:no_of_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lm_fit3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Multiple R-squared is 0.3659. It increase 0.05.And at the same time we have a larger Adjusted R-squared. Our added featured absolutely are helpful.\n",
    "- There are also some added value with large p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_fit4<- update(lm_fit2,.~ .+quarter:no_of_workers+department:no_of_workers+smv:no_of_workers+department:log(incentive+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(lm_fit4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete unimportant variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the above analysis, we finally manually input our final model. I DON'T t know why some features are reserved here.In addition to the values left by EDA, there are some values that I cannot explain. For example, smv and log (smv) must be left at the same time to have better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = lm(formula = actual_productivity ~ quarter + department + targeted_productivity + \n",
    "    smv + over_time + idle_men + no_of_workers + \n",
    "    log(targeted_productivity) + log(smv + 1) + quarter:no_of_workers + \n",
    "    department:incentive + department:no_of_workers + targeted_productivity:no_of_workers + \n",
    "    smv:no_of_workers, data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step =step(fin,trace=0,k=log(nrow(train)), direction=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "par(mfrow = c(2,2))\n",
    "plot(step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Residuals vs Fitted shows redisuals are more lying around a red line in figure. We can compare it with figure for lm_fit1 above, Obviously, the points in the upper half are closer to a straight line. That means Residuals and Fitted are more linear here.\n",
    "\n",
    "- Q-Q norm shows our model are less like a normal distribution. Instead, it is a model of the long tail on the left.\n",
    "\n",
    "- We can see the residuals spread equally along the red line from scale-location. We don't know how equally it is,but it looks like quiet equally.That means our homoscedaslticity is OK.\n",
    "\n",
    "- Residuals vs Leverage shows, we still have some points which are influential points. But basically, points lie at left side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principle of logistic is more complicated. It is a method formed by simple Bayesian and gradient descent iteration. Here we try to use logistic to do regression. The features are still the features we have sorted out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_fit<- glm(formula = actual_productivity ~ quarter + department + targeted_productivity + \n",
    "    smv + over_time + idle_men + no_of_workers + \n",
    "    log(targeted_productivity) + log(smv + 1) + quarter:no_of_workers + \n",
    "    department:incentive + department:no_of_workers + targeted_productivity:no_of_workers + \n",
    "    smv:no_of_workers, data = train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(glm_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw_flm_fit =step(glm_fit,trace=0,k=log(nrow(train)), direction=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(sw_flm_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks the same as simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN which is K Nearest Neibhours. It pridict value by their nearest neibhours. We use euclealian distance to calculate the distance.\n",
    "\n",
    "\\begin{equation}\n",
    "euclealian \\ distance = \\sqrt{\\sum_{i=1}^{N}(x_i-y_i)^2} \n",
    "\\end{equation}\n",
    "\n",
    "In order to compairing different model,we keep same features in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=trainControl(method='cv',number=10)\n",
    "knn_fit=train(actual_productivity ~ quarter + department + targeted_productivity + \n",
    "    smv + over_time + idle_men + no_of_workers + \n",
    "    log(targeted_productivity) + log(smv + 1) + quarter:no_of_workers + \n",
    "    department:incentive + department:no_of_workers + targeted_productivity:no_of_workers + \n",
    "    smv:no_of_workers, data = train,method ='knn',\n",
    "             trcontrol=count,\n",
    "             tuneGrid=expand.grid(k=1:20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(knn_fit,main=\"K vs RMSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From figure above, we know when K = 11, we have lowest RMSE. So, we choose K =11.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=trainControl(method='cv',number=10)\n",
    "knn_fin=train(actual_productivity ~ quarter + department + targeted_productivity + \n",
    "    smv + over_time + idle_men + no_of_workers + \n",
    "    log(targeted_productivity) + log(smv + 1) + quarter:no_of_workers + \n",
    "    department:incentive + department:no_of_workers + targeted_productivity:no_of_workers + \n",
    "    smv:no_of_workers, data = train,method ='knn',\n",
    "             trcontrol=count,\n",
    "             tuneGrid=expand.grid(k=11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision TREE(XgbTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02:14:11] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[02:14:11] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[02:14:12] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[02:14:12] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[02:14:12] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[02:14:13] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[02:14:13] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[02:14:13] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[02:14:13] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[02:14:14] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n",
      "[02:14:14] WARNING: amalgamation/../src/objective/regression_obj.cu:171: reg:linear is now deprecated in favor of reg:squarederror.\n"
     ]
    }
   ],
   "source": [
    "tune_grid <- expand.grid(nrounds = 50,\n",
    "                        max_depth = 5,\n",
    "                        eta = 0.05,\n",
    "                        gamma = 0.01,\n",
    "                        colsample_bytree = 0.75,\n",
    "                        min_child_weight = 0,\n",
    "                        subsample = 0.5)\n",
    "\n",
    "\n",
    "xgb_model <- train(actual_productivity ~ quarter + department + targeted_productivity + \n",
    "    smv + over_time + idle_men + no_of_workers + \n",
    "    log(targeted_productivity) + log(smv + 1) + quarter:no_of_workers + \n",
    "    department:incentive + department:no_of_workers + targeted_productivity:no_of_workers + \n",
    "    smv:no_of_workers, data = train, method = \"xgbTree\",trControl = trainControl(\"cv\", number = 10), tuneGrid = tune_grid,\n",
    "                tuneLength = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use MSE to calculate error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we know Model of Mutiple linear regression is same as Logistic one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(step, train):\n",
      "\"prediction from a rank-deficient fit may be misleading\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>RMSE</dt>\n",
       "\t\t<dd>0.122203561424824</dd>\n",
       "\t<dt>Rsquared</dt>\n",
       "\t\t<dd>0.495218917566995</dd>\n",
       "\t<dt>MAE</dt>\n",
       "\t\t<dd>0.0814135731365613</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[RMSE] 0.122203561424824\n",
       "\\item[Rsquared] 0.495218917566995\n",
       "\\item[MAE] 0.0814135731365613\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "RMSE\n",
       ":   0.122203561424824Rsquared\n",
       ":   0.495218917566995MAE\n",
       ":   0.0814135731365613\n",
       "\n"
      ],
      "text/plain": [
       "      RMSE   Rsquared        MAE \n",
       "0.12220356 0.49521892 0.08141357 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in predict.lm(step, test):\n",
      "\"prediction from a rank-deficient fit may be misleading\""
     ]
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>RMSE</dt>\n",
       "\t\t<dd>0.135204337717158</dd>\n",
       "\t<dt>Rsquared</dt>\n",
       "\t\t<dd>0.438424367980672</dd>\n",
       "\t<dt>MAE</dt>\n",
       "\t\t<dd>0.0893595011212676</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[RMSE] 0.135204337717158\n",
       "\\item[Rsquared] 0.438424367980672\n",
       "\\item[MAE] 0.0893595011212676\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "RMSE\n",
       ":   0.135204337717158Rsquared\n",
       ":   0.438424367980672MAE\n",
       ":   0.0893595011212676\n",
       "\n"
      ],
      "text/plain": [
       "     RMSE  Rsquared       MAE \n",
       "0.1352043 0.4384244 0.0893595 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "postResample(pred = predict(step,train),obs=train$actual_productivity)\n",
    "postResample(pred = predict(step,test),obs=test$actual_productivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>RMSE</dt>\n",
       "\t\t<dd>0.139256289202309</dd>\n",
       "\t<dt>Rsquared</dt>\n",
       "\t\t<dd>0.350692205559129</dd>\n",
       "\t<dt>MAE</dt>\n",
       "\t\t<dd>0.0982030403299156</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[RMSE] 0.139256289202309\n",
       "\\item[Rsquared] 0.350692205559129\n",
       "\\item[MAE] 0.0982030403299156\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "RMSE\n",
       ":   0.139256289202309Rsquared\n",
       ":   0.350692205559129MAE\n",
       ":   0.0982030403299156\n",
       "\n"
      ],
      "text/plain": [
       "      RMSE   Rsquared        MAE \n",
       "0.13925629 0.35069221 0.09820304 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>RMSE</dt>\n",
       "\t\t<dd>0.159146816838808</dd>\n",
       "\t<dt>Rsquared</dt>\n",
       "\t\t<dd>0.231433147897928</dd>\n",
       "\t<dt>MAE</dt>\n",
       "\t\t<dd>0.113008004423669</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[RMSE] 0.159146816838808\n",
       "\\item[Rsquared] 0.231433147897928\n",
       "\\item[MAE] 0.113008004423669\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "RMSE\n",
       ":   0.159146816838808Rsquared\n",
       ":   0.231433147897928MAE\n",
       ":   0.113008004423669\n",
       "\n"
      ],
      "text/plain": [
       "     RMSE  Rsquared       MAE \n",
       "0.1591468 0.2314331 0.1130080 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "postResample(pred = predict(knn_fin,train),obs=train$actual_productivity)\n",
    "postResample(pred = predict(knn_fin,test),obs=test$actual_productivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xgbTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>RMSE</dt>\n",
       "\t\t<dd>0.107324730139431</dd>\n",
       "\t<dt>Rsquared</dt>\n",
       "\t\t<dd>0.653346659477759</dd>\n",
       "\t<dt>MAE</dt>\n",
       "\t\t<dd>0.0757113088986176</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[RMSE] 0.107324730139431\n",
       "\\item[Rsquared] 0.653346659477759\n",
       "\\item[MAE] 0.0757113088986176\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "RMSE\n",
       ":   0.107324730139431Rsquared\n",
       ":   0.653346659477759MAE\n",
       ":   0.0757113088986176\n",
       "\n"
      ],
      "text/plain": [
       "      RMSE   Rsquared        MAE \n",
       "0.10732473 0.65334666 0.07571131 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<dl class=dl-horizontal>\n",
       "\t<dt>RMSE</dt>\n",
       "\t\t<dd>0.130449619731694</dd>\n",
       "\t<dt>Rsquared</dt>\n",
       "\t\t<dd>0.489957513258076</dd>\n",
       "\t<dt>MAE</dt>\n",
       "\t\t<dd>0.0903577966542249</dd>\n",
       "</dl>\n"
      ],
      "text/latex": [
       "\\begin{description*}\n",
       "\\item[RMSE] 0.130449619731694\n",
       "\\item[Rsquared] 0.489957513258076\n",
       "\\item[MAE] 0.0903577966542249\n",
       "\\end{description*}\n"
      ],
      "text/markdown": [
       "RMSE\n",
       ":   0.130449619731694Rsquared\n",
       ":   0.489957513258076MAE\n",
       ":   0.0903577966542249\n",
       "\n"
      ],
      "text/plain": [
       "     RMSE  Rsquared       MAE \n",
       "0.1304496 0.4899575 0.0903578 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "postResample(pred = predict(xgb_model,train),obs=train$actual_productivity)\n",
    "postResample(pred = predict(xgb_model,test),obs=test$actual_productivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the RMSE, R squared, and AME of the three models, it is clear that xgbTree is the best performer. xgbTree has the smallest RMSE, and AME, and at the same time has a larger R squared. It's just that we only let xgbTree iterate 50 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results and discussion <a class=\"anchor\" id=\"sec_6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First of all, for a machine learning model, the smaller the gap between its prediction and the true value, the better the model's performance. After the above calculation, we know by judging RMSE and AME that these two values ​​of xgb_model are relatively small. This shows that the difference between the predicted value and the true value of xgb_model is the smallest.\n",
    "\n",
    "- Secondly, xgb_model behaves like this on both train and test datasets.\n",
    "\n",
    "- Finally, the value of R squared of xgb_model is the largest. First of all, this shows that the features in the model can explain more prediction variance. In addition, because RMSE and AME are both low levels, this shows that this is not caused by the features we have used.\n",
    "\n",
    "Based on the above analysis, if I can only choose one, I will choose xgb_model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset of Attributes Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first is the sub attriabutes obtained from EDA analysis.\n",
    "\n",
    "1. smv:no_of_workers，targeted_productivity:no_of_workers \n",
    "\n",
    "The first known information is that in addition to target_productivity, there is no attribute and actual_Productivity\n",
    "There is relevance. So in the EDA part, we calculated the correlation between the two for digital data. There are strong correlations among several attributes. So we guess that after combining them in pairs, maybe it will play a big role in the prediction of the model.\n",
    "\n",
    "2. log(targeted_productivity) , log(smv + 1)\n",
    "\n",
    "First, after observing the distribution of actual_productivity, we found that it seems to be a Gauss-like distribution.\n",
    "In the EDA section, when we observe several log-scaled attributes, we find that the incidence is approximately a normal distribution after doing the log. In addition, after logging smv, it was found that it became concentrated in two parts. So we can reasonably guess that logging them in machine learning may have a good effect. log(target_productivity) is completely unfounded. And it seems that when log and non-log exist at the same time, it performs better.\n",
    "\n",
    "3. quarter:no_of_workers , department:incentive , department:no_of_workers\n",
    "\n",
    "In the EDA part, we found that when no_of_workers is distributed in different quarters, the main distribution of product rate has changed a lot. We have reason to suspect that quarter5 may lead to a higher productivity. Similarly, the distribution of different department states and no_of_workers, different department states and incentives also seems to affect the distribution of actual_productivity. In other words, after the current combination of the two, in general, we can predict which value the actual_productivity will probably be distributed around. So we have reason to suspect that these values can become sub attributes.As for log (incentive): department, we found that his performance is not as good as incentive: department.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing machine learning, it is very important to analyze the data first. We can not only have a macro understanding of the entire data, but more importantly, it is of great help to our selection of attributes. As shown in the GEP, almost no attributes are related to the target value. This makes us have to think of ways to transform or combine attributes. If we use fancy transformation, and we let all attributes take the log or combine them in pairs, our model will be very complicated. Even if we do pruning later, this is impossible when the data is extremely complex. Therefore, it is very important to try to find the potential relationship between variables in EDA.\n",
    "\n",
    "When doing machine learning, we have many models to choose from. In addition to simple basic models such as lm and knn, there are many models that have been edited by data scientists like xgboost. Choosing a better model can directly improve the accuracy of the model. In addition, when choosing a model, we must also consider the goals we want to achieve. Generally speaking, we hope that the more accurate the better, but in addition to regression, there are problems such as classification and clustering. These issues are worthy of in-depth consideration before choosing a more appropriate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion<a class=\"anchor\" id=\"sec_7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, we use different visualization methods to analyze the data for different data types (num, str). We focus on exploring potential sub attributes based on correlation calculations. We also draw a log_scale graph, aiming at the density map of the target value to explore which combined variables may be related to the target. In addition, we use different models for machine learning. Multiple linear regression, logistic regression, KNN, and XGB Tree are all used. We first constantly update the model to prove that the sub attributes we conjecture can indeed make the model perform better. Then, we compare the performance between different models. This performance is mainly achieved by comparing RMSE, AME, and R squared. In the end we choose xgb_model. \n",
    "\n",
    "\n",
    "Of course, there are also many shortcomings. For example, we ignore the impact of dates. For example, there must be better sub attributes, but we haven't found them.This requires more research from us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. References <a class=\"anchor\" id=\"sec_8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Persistent invalid graphics state error when using ggplot2 https://stackoverflow.com/questions/20155581/persistent-invalid-graphics-state-error-when-using-ggplot2\n",
    "\n",
    "- How to Calculate Residual Standard Error in R https://www.statology.org/residual-standard-error-r/\n",
    "\n",
    "- knn: k-Nearest Neighbour Classification https://www.rdocumentation.org/packages/class/versions/7.3-19/topics/knn\n",
    "\n",
    "- trainControl: Control parameters for train https://www.rdocumentation.org/packages/caret/versions/6.0-88/topics/trainControl\n",
    "\n",
    "- How to plot XGBoost trees in R https://www.r-bloggers.com/2021/04/how-to-plot-xgboost-trees-in-r/\n",
    "\n",
    "- Simple R - xgboost - caret kernel https://www.kaggle.com/nagsdata/simple-r-xgboost-caret-kernel\n",
    "\n",
    "- The caret Package Max Kuhn 2019-03-27 https://topepo.github.io/caret/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
